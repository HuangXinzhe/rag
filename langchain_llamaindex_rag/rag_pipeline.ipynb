{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LangChain和LlamaIndex从零构建PDF聊天机器人\n",
    "具体步骤：\n",
    "- 加载文档（PDF、HTML、文本、数据库等）\n",
    "- 将数据分割成块，并对这些块建立embedding索引，这样方便使用向量检索工具进行语义搜索\n",
    "- 对于每个问题，通过搜索索引和embedding数据来获取与问题相关的信息\n",
    "- 将问题和相关数据输入到LLM模型中。在这个系列中使用OpenAI的LLM；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于使用的是ChatGPT，需要获得OpenAI的API密钥。请访问[OpenAI](https://beta.openai.com/signup/)注册并获取API密钥。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_base = os.getenv(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='A Survey on Retrieval-Augmented Text Generation\\nHuayang Li~;\\x03Yixuan Su\\x7f;\\x03Deng Cai};\\x03Yan Wang|;\\x03Lemao Liu|;\\x03\\n~Nara Institute of Science and Technology\\x7fUniversity of Cambridge\\n}The Chinese University of Hong Kong|Tencent AI Lab\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community. Compared\\nwith conventional generation models, retrieval-\\naugmented text generation has remarkable ad-\\nvantages and particularly has achieved state-of-\\nthe-art performance in many NLP tasks. This\\npaper aims to conduct a survey about retrieval-\\naugmented text generation. It ﬁrstly highlights\\nthe generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable ap-\\nproaches according to different tasks including\\ndialogue response generation, machine trans-\\nlation, and other generation tasks. Finally, it', metadata={'source': '/Users/huangxinzhe/论文阅读/RAG/2202.01110.pdf', 'page': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the PDF using pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# load the data\n",
    "loader = PyPDFLoader('/Users/huangxinzhe/论文阅读/RAG/2202.01110.pdf')\n",
    "\n",
    "# the 10k financial report are huge, we will need to split the doc into multiple chunk.\n",
    "# This text splitter is the recommended one for generic text. It is parameterized by a list of characters. \n",
    "# It tries to split on them in order until the chunks are small enough.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "data = loader.load()\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "# view the first chunk\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/rag10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# import Chroma and OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# initialize OpenAIEmbedding\n",
    "# embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# use Chroma to create in-memory embedding database from the doc\n",
    "docsearch = Chroma.from_documents(texts, embeddings,  metadatas=[{\"source\": str(i)} for i in range(len(texts))])\n",
    "\n",
    "## perform search based on the question\n",
    "query = \"What is the operating income?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='decoding is that it does not require to change the\\ntranslation model (including phrase table and pa-\\nrameters) and can be applied in a plug-and-play\\nway. This approach is successful when xis highly\\nsimilar to xr; otherwise its performance is de-\\ngraded largely, because it explicitly isolates TMmatching and SMT decoding and reuses the results\\ninxror not in a deterministic way.\\nPhrase Table Aggregation with TM There are\\nalso notable efforts to augment the phrase table\\nfor SMT by extracting translation rules from the\\nretrieved bilingual sentences fhxr;yrig. Then\\nthey re-tune the parameters for the SMT model\\nwhich makes use of translation knowledge from\\nfhxr;yrigin a implicit way when translating x.\\nFor example, Biçici and Dymetman (2008); Simard\\nand Isabelle (2009) directly combine the extracted\\ntranslation rules into the phrase table in a shallow\\ncombination way. They introduce an additional fea-\\nture to indicate that whether translation rule is from', metadata={'source': '/Users/huangxinzhe/论文阅读/RAG/2202.01110.pdf', 'page': 4}),\n",
       " Document(page_content='metrics for retrieval. This can be beneﬁcial for\\nmore controlled text generation. For example, in-\\nstances with emotions and styles may be more de-\\nsirable in the personalized dialogue generation, par-\\nallel data that contains speciﬁc terminologies is\\nmore helpful in machine translation, and so on. On\\nthe other hand, using a universal metric for retrieval\\nmay lead to the lack of diversity of the retrieval re-\\nsults. Collecting a diverse set of retrieval results\\ncan improve the coverage of useful information.\\nThus, considering multiple different metrics for re-\\ntrieval may lead to generation with higher quality\\nin the future.', metadata={'source': '/Users/huangxinzhe/论文阅读/RAG/2202.01110.pdf', 'page': 7}),\n",
       " Document(page_content='Input\\nSources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific RetrievalRetrieval MemoryGeneration ModelSec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & TemplatesInformation RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4):OutputFigure 1: The overview of this survey.\\ntrieval source, retrieval metric and integration meth-\\nods.\\n2.1 Formulation\\nMost text generation tasks can be formulated as a\\nmapping from input sequence xto output sequence\\ny:y=f(x). For instance, xandycould be the\\ndialogue history and the corresponding response\\nfor dialogue response generation, the text in the\\nsource language and the translation in the target\\nlanguage for machine translation, and so on.\\nRecently, some researchers suggest to endow\\nmodels the capability to access external memory\\nvia some information retrieval techniques, so that', metadata={'source': '/Users/huangxinzhe/论文阅读/RAG/2202.01110.pdf', 'page': 1}),\n",
       " Document(page_content='International Workshop on Example-Based Machine\\nTranslation , pages 3–10. Citeseer.', metadata={'source': '/Users/huangxinzhe/论文阅读/RAG/2202.01110.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain提供了四种预先构建的问答Chain，具体如下：\n",
    "\n",
    "- 问答：load_qa_chain\n",
    "\n",
    "- 有来源问答：load_qa_with_sources_chain\n",
    "\n",
    "- 检索问题答案：RetrievalQA\n",
    "\n",
    "- 资源检索问答：RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing necessary framework\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问答：load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use LLM to get answering\n",
    "chain = load_qa_chain(ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), \n",
    "                      chain_type=\"stuff\")\n",
    "query = \"What is the operating income?\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有来源问答：load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), \n",
    "                                   chain_type=\"stuff\")\n",
    "query = \"What is the operating income?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检索问题答案：RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), \n",
    "                               chain_type=\"stuff\", \n",
    "                               retriever=docsearch.as_retriever())\n",
    "query = \"What is the operating income?\"\n",
    "qa.run(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 资源检索问答：RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=RetrievalQAWithSourcesChain.from_chain_type(ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), \n",
    "                                                  chain_type=\"stuff\", \n",
    "                                                  retriever=docsearch.as_retriever())\n",
    "chain({\"question\": \"What is the operating income?\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex是一个用于构建和搜索向量索引的库。它可以用于构建和搜索向量索引，以便在大型数据集中进行快速的相似性搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "## setup your OpenAI Key\n",
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# enable logs to see what happen underneath\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 4 (char 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_loader\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# we will use this UnstructuredReader to read PDF file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m UnstructuredReader \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUnstructuredReader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m loader \u001b[38;5;241m=\u001b[39m UnstructuredReader()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# load the data\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag10/lib/python3.10/site-packages/llama_index/readers/download.py:133\u001b[0m, in \u001b[0;36mdownload_loader\u001b[0;34m(loader_class, loader_hub_url, refresh_cache, use_gpt_index_import, custom_path)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loader_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     library_raw_content, _ \u001b[38;5;241m=\u001b[39m _get_file_content(loader_hub_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/library.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m     library \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_raw_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loader_class \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m library:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoader class name not found in library\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag10/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rag10/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 4 (char 3)"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "from llama_index import download_loader\n",
    "\n",
    "# we will use this UnstructuredReader to read PDF file\n",
    "UnstructuredReader = download_loader('UnstructuredReader', refresh_cache=True)\n",
    "loader = UnstructuredReader()\n",
    "# load the data\n",
    "data = loader.load_data(f'/Users/huangxinzhe/论文阅读/RAG/2202.01110.pdf', split_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
